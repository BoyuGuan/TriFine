# TriFine Dataset and FIAT Framework
This repository contains the official implementation for our COLING 2025 paper:

[TriFine: A Large-Scale Dataset of Vision-Audio-Subtitle for Tri-Modal Machine Translation and Benchmark with Fine-Grained Annotated Tags](https://aclanthology.org/2025.coling-main.547/)

## Dataset and Code Release

We appreciate your patience! The dataset construction details and model codes are now available in this repository.

## Data Access and Usage

Due to commitments to reviewers regarding legal compliance and copyright concerns, we kindly request researchers interested in using the TriFine Dataset to apply by sending an email to guanboyu2022@ia.ac.cn. Your email must fulfill the following requirements:

1. Sent from an email address ending with .edu.

2. Include proof of your affiliation with the educational institution associated with the .edu email (such as a student or faculty ID card). You may obscure sensitive information; we will only use the provided proof for compliance verification purposes and will never misuse or distribute your personal information.

3. Explicitly confirm that you will use the dataset strictly for academic purposes.

Thank you for your understanding and cooperation!

## Citation
If you want to cite our paper, please use the following BibTex entries:
```BibTex
@inproceedings{guan-etal-2025-trifine,
    title = "{T}ri{F}ine: A Large-Scale Dataset of Vision-Audio-Subtitle for Tri-Modal Machine Translation and Benchmark with Fine-Grained Annotated Tags",
    author = "Guan, Boyu  and
      Zhang, Yining  and
      Zhao, Yang  and
      Zong, Chengqing",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.547/",
    pages = "8215--8231",
    abstract = "Current video-guided machine translation (VMT) approaches primarily use coarse-grained visual information, resulting in information redundancy, high computational overhead, and neglect of audio content. Our research demonstrates the significance of fine-grained visual and audio information in VMT from both data and methodological perspectives. From the data perspective, we have developed a large-scale dataset TriFine, the first vision-audio-subtitle tri-modal VMT dataset with annotated multimodal fine-grained tags. Each entry in this dataset not only includes the triples found in traditional VMT datasets but also encompasses seven fine-grained annotation tags derived from visual and audio modalities. From the methodological perspective, we propose a Fine-grained Information-enhanced Approach for Translation (FIAT). Experimental results have shown that, in comparison to traditional coarse-grained methods and text-only models, our fine-grained approach achieves superior performance with lower computational overhead. These findings underscore the pivotal role of fine-grained annotated information in advancing the field of VMT."
}
```
